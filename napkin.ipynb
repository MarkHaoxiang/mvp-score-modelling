{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from diffusers import ScoreSdeVePipeline, ScoreSdeVeScheduler, UNet2DModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'     # Update this line if you want to use a different device such as TPU or Macbook's MPS\n",
    "PRETRAINED = \"google/ncsnpp-celebahq-256\"\n",
    "DATASET_SOURCE = \"Ryan-sjtu/celebahq-caption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "unconditional_pipeline = ScoreSdeVePipeline.from_pretrained(PRETRAINED)\n",
    "scheduler = ScoreSdeVeScheduler.from_pretrained(PRETRAINED)\n",
    "unet = UNet2DModel.from_pretrained(PRETRAINED)\n",
    "dataset: DatasetDict = load_dataset(DATASET_SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a time dependent gender classifier\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# EfficientNet\n",
    "weights = MobileNet_V3_Small_Weights.DEFAULT\n",
    "model = mobilenet_v3_small(weights=weights)\n",
    "model = nn.Sequential(\n",
    "    model.features,\n",
    "    model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(0.2, inplace=True),\n",
    "    nn.Linear(in_features=576, res=1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model = model.to(device=DEVICE)\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Dataset\n",
    "train_ds: Dataset = dataset['train'].with_format('torch', device=DEVICE)\n",
    "train_ds = train_ds.map(lambda x: {\n",
    "    'input': preprocess(torch.transpose(x['image'], -1,-3)),\n",
    "    'label': torch.ones(1, device=DEVICE) if 'woman' in x['text'] else torch.zeros(1, device=DEVICE)\n",
    "})\n",
    "dataloader = DataLoader(train_ds, 64)\n",
    "\n",
    "# TODO: Noise timescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCELoss()\n",
    "sig = nn.Sigmoid().to(device=DEVICE)\n",
    "\n",
    "with tqdm(total=N_EPOCHS) as pbar:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, labels = data['input'], data['label']\n",
    "            optim.zero_grad()\n",
    "            predicted = model(inputs)\n",
    "            loss = loss_fn(predicted, labels)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        # Logging\n",
    "        pbar.set_description(f\"loss {running_loss}\")\n",
    "        pbar.update(1)\n",
    "        pbar.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 12\n",
    "test_ds: Dataset = dataset['train'].with_format('torch', device=DEVICE)\n",
    "img = test_ds[id]['image']\n",
    "img = preprocess(torch.transpose(img,-3,-1))\n",
    "print(img)\n",
    "print(f\"Predicted {model(torch.unsqueeze(img, 0))}\")\n",
    "plt.imshow(test_ds[id]['image'].cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
