{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\ml_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SOURCE = \"Ryan-sjtu/celebahq-caption\"\n",
    "\n",
    "dataset = load_dataset(DATASET_SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 13500, Test size: 1500\n"
     ]
    }
   ],
   "source": [
    "train_percent = 0.45\n",
    "test_percent = 0.05\n",
    "\n",
    "train_size = int(len(dataset[\"train\"]) * train_percent)\n",
    "test_size = int(len(dataset[\"train\"]) * test_percent)\n",
    "print(f\"Train size: {train_size}, Test size: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 30000/30000 [00:07<00:00, 4255.21 examples/s] \n"
     ]
    }
   ],
   "source": [
    "def label_dataset(batch):\n",
    "    # `batch` is a dictionary where each value is a list of entries\n",
    "    texts = batch['text']\n",
    "    # Apply the label logic to each entry in the batch\n",
    "    batch['label'] = [1 if 'woman' in text.lower() else 0 for text in texts]\n",
    "    return batch\n",
    "\n",
    "ds = dataset['train'].map(label_dataset, batched=True, batch_size=32, remove_columns=['text'], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 30000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, value):\n",
    "    return [x for x in batch if x['label'] == value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 size: 10966, Dataset 1 size: 19034\n"
     ]
    }
   ],
   "source": [
    "dataset_0 = ds.filter(lambda batch: [x==0 for x in batch['label']], batched=True, batch_size=32, num_proc=4)\n",
    "dataset_1 = ds.filter(lambda batch: [x==1 for x in batch['label']], batched=True, batch_size=32, num_proc=4)\n",
    "print(f\"Dataset 0 size: {len(dataset_0)}, Dataset 1 size: {len(dataset_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_size, test_size):\n",
    "    split_dataset = dataset.train_test_split(test_size=test_size)\n",
    "    train_dataset = split_dataset['train'].train_test_split(train_size=train_size / (1 - test_size))\n",
    "    return train_dataset['train'], train_dataset['test'], split_dataset['test']\n",
    "\n",
    "train_dataset_0, _, test_dataset_0 = split_dataset(dataset_0, train_percent, test_percent)\n",
    "train_dataset_1, _, test_dataset_1 = split_dataset(dataset_1, train_percent, test_percent)\n",
    "\n",
    "# Combine \n",
    "train_dataset = concatenate_datasets([train_dataset_0, train_dataset_1])\n",
    "test_dataset = concatenate_datasets([test_dataset_0, test_dataset_1])\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=0)\n",
    "test_dataset = test_dataset.shuffle(seed=0)\n",
    "\n",
    "final_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 13499\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1501\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted ../output/celebahq/test/all\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"../output/celebahq/test/all\"\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir)\n",
    "    print(f\"Deleted {save_dir}\")\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Documents\\George\\Python_Projects\\mvp-score-modelling\\notebooks\\dataset-test.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/George/Python_Projects/mvp-score-modelling/notebooks/dataset-test.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m save_image\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/George/Python_Projects/mvp-score-modelling/notebooks/dataset-test.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m save_image_function \u001b[39m=\u001b[39m create_save_image_function(save_dir)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/George/Python_Projects/mvp-score-modelling/notebooks/dataset-test.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_dataset\u001b[39m.\u001b[39mmap(save_image_function, with_indices\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batched\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_proc\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def create_save_image_function(save_directory):\n",
    "    # create closure to encapsulate save_directory\n",
    "    def save_image(example, index):\n",
    "        import os\n",
    "        image = example['image']\n",
    "        filepath = os.path.join(save_directory, f'image_{index}.png')\n",
    "        image.save(filepath)\n",
    "        return example  # Return the unmodified example\n",
    "    return save_image\n",
    "\n",
    "save_image_function = create_save_image_function(save_dir)\n",
    "\n",
    "test_dataset.map(save_image_function, with_indices=True, batched=False, num_proc=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
